 
Aperture problem of the optical flow algorithms - having 2 unknowns:
 
Methods for determination:
Phase correlation – inverse of normalized cross-power spectrum
Block-based methods – minimizing sum of squared differences or sum of absolute differences, or maximizing normalized cross-correlation
Differential methods of estimating optical flow, based on partial derivatives of the image signal and/or the sought flow field and higher-order partial derivatives, such as:
Lucas–Kanade method – regarding image patches and an affine model for the flow field[10]
Horn–Schunck method – optimizing a functional based on residuals from the brightness constancy constraint, and a particular regularization term expressing the expected smoothness of the flow field[10]
Buxton–Buxton method – based on a model of the motion of edges in image sequences[11]
Black–Jepson method – coarse optical flow via correlation[7]
General variational methods – a range of modifications/extensions of Horn–Schunck, using other data terms and other smoothness terms.
Discrete optimization methods – the search space is quantized, and then image matching is addressed through label assignment at every pixel, such that the corresponding deformation minimizes the distance between the source and the target image.[12] The optimal solution is often recovered through Max-flow min-cut theorem algorithms, linear programming or belief propagation methods.

Phase correlation - calculated by fast Fourier transforms

Hann and Hamming windows:
window function

cross-power spectrum

-----
IBR - Image-based rendering
image-based modeling and rendering (IBMR) - methods rely on a set of two-dimensional images of a scene to generate a three-dimensional model and then render some novel views of this scene.

----------
View synthesis

Depth-image-based rendering (DIBR) 

-----------
Free-Viewpoint video (FVV)
Free-viewpoint TV (FTV) 
3D Audio Visual (3DAV)

Phase 1 - Multi-view Video Coding (MVC) - initiated in 2004 and completed in 2009. 
Phase 2 - 3D Video (3DV) - 2007 and just completed recently
Phase 3 - July 2014 - now

Moving Picture Experts Group (MPEG) has been conducting Free-viewpoint TV (FTV) standardization [3] since 2001. In 2001, FTV was proposed to MPEG and the corresponding 3D Audio Visual (3DAV) activity started. The first phase of FTV, which is Multi-view Video Coding (MVC), was initiated in 2004 and completed in 2009. The second phase of FTV, which is known as 3D Video (3DV), started in 2007 and just completed recently. In the recent MPEG meetings since July 2014, FTV has started a new round of exploration experiments [4] for the third phase of FTV.

Blue-ray 3D, which involves only (texture/color) video of multiple views and exploits the correlation among different views to further enhance the coding efficiency.

The current 3DV involves both texture videos and the corresponding depth videos of multiple views, e.g., 3 views of texture and depth videos in the recommended configuration. These views are sent in the encoder side while a larger number of views can be generated at the receiver side based on these views by employing view synthesis. It generally targets the multiview displays with views less than about 30. 

The newly started FTV exploration experiment aims at two specific applications, super multiview video and free viewpoint navigation, which requires the system to be able to synthesize dense views and specified views. Both the current 3DV and the newly started FTV need to provide virtual views using the received multiple views (decoded on the user side), which makes the view synthesis a key component in the processing chain of the 3D video system.

hole filling
image inpainting
temporal correlation information to fill up holes

--- 
Dense motion

Computer Vision: Algorithms and Applications. R Szeliski, Ch7. 

OLD: frame-rate image alignment
OLD: patch-based translational alignment (optical flow) technique developed by Lucas and Kanade (1981).
OLD: used in all motion-compensated video compression schemes such as MPEG and H.263 (Le Gall 1991). 

parametric motion estimation algorithms have found a wide variety
of applications:
- video summarization (Bergen et al. 1992a, Teodosio and Bender 1993,
Kumar et al. 1995, Irani and Anandan 1998)
- video stabilization (Hansen et al. 1994, Srinivasan et
al. 2005, Matsushita et al. 2006)
- video compression (Irani et al. 1995, Lee et al. 1997)

To estimate the motion between two or more images, a suitable error metric must first be
chosen to compare the images (§7.1).

Estimage motion - either:
- full search
- hierarchical coarse-to-fine techniques (§7.1.1) based on image pyramids
- Fourier transforms

Sub-pixel precision in the alignment:

- incremental methods (§7.1.3) based on a Taylorseries expansion of the image function:
--applied to parametric motion models (§7.2), which model global image transformations such as rotation or shearing

- piecewise parametric spline motion models (§7.3) can be used. 
- If pixel accurate correspondences are desired, general-purpose  - optical flow (aka optic flow) techniques
have been developed (§7.4)

- For more complex motions that include a lot of occlusions, layered motion models (§7.5), which decompose the scene into coherently moving layers, can work well.

- Finally, motion estimation can also be made more reliable by learning the typical dynamics or
motion statistics of the scenes or objects being tracked, e.g., the natural gait of walking people

The assumption that corresponding pixel values remain the same in the two images is often called the
brightness constancy assumption.

brightness constancy assumption

--
Color images can be processed by summing differences across all three color channels, although it is also possible to first transform the images into a different color space or to only use the luminance (which is often done in video
encoders).

---
7.1 Translational alignment
Error estimated based on:

- Spatially varying weights
 - sum of absolute differences (SAD)
 - sum of squared differences (SSD)
 - Geman-McClure function
 - Weighted (Windowed) SSD functon

If images do not overlap - or to remove backgound:
All of these tasks can be accomplished by associating a spatially varying per-pixel weight
value with each of the two images being matched. The error metric then become the weighted (or
windowed) SSD function,
EWSSD(u) = Sum(i) of w0(xi)w1(xi + u)[I1(xi + u) − I0(xi)]squared
where the weighting functions w0 and w1 are zero outside the valid ranges of the images.

 - Divide by overlap window:
Per-pixel (or mean) squared pixel error. The square root of this quantity is the root
mean squared intensity error
RMS = square root (EWSSD/A )

-Bias and gain model - β is the bias and α is the gain - 
EBG(u) = Sum of (i) [I1(xi + u) − (1 + α)I0(xi) − β]squared = Sum of (i)[αI0(xi) + β − ei]squared 

-Correlation
  - Normalized cross-correlation (NCC)

7.1.1 Hierarchical motion estimation

 - FULL search - block matching in motion compensated video compression, where a range of possible motions (say ±16 pixels) is explored.

 - hierarchical motion estimation (see Bergen et al. 1992a)

7.1.2 Fourier-based alignment

Requires only O(NM log NM) operations (Bracewell 1986) - as opposed to O(N2M2) operations for Full search

- Fourier
- Windowed correlation. Unfortunately, the Fourier convolution theorem only applies when the
summation over xi
is performed over all the pixels in both images, using a circular shift of the
image when accessing pixels outside the original boundaries. While this is acceptable for small
shifts and comparably sized images, it makes no sense when the images overlap by a small amount
or one image is a small subset of the other.
In that case, the cross-correlation function should be replaced with a windowed (weighted)
cross-correlation function

- Phase correlation. A variant of regular correlation (7.18) that is sometimes used for motion
estimation is phase correlation (Kuglin and Hines 1975, Brown 1992). 
Outperforms - depeninding on image quality. Not good for blurry images.

 - Recently, gradient cross-correlation has emerged as a promising alternative to phase correlation
(Argyriou and Vlachos 2003), although further systematic studies are probably warranted. Phase
correlation has also been studied by Fleet and Jepson (1990) as a method for estimating general
optical flow and stereo disparity

- Rotations and scale estimation also possible - this trick only applies when the images have large overlap (small translational
motion).

ABOVE - estimate translational alignment to the nearest pixel

7.1.3 Incremental refinement

Sub-pixel:

Lucas and Kanade (1981), is to do gradient descent on the SSD energy function (7.1), using a Taylor Series expansion of the image function

Matrices are called the (Gauss-Newton approximation of the) Hessian and gradient-weighted residual vector,
respectively...

 - Uncertainty modeling
covariance matrix, which captures the expected variance in the motion estimate in all possible directions

For larger amounts of noise, the linearization performed by the Lucas-Kanade
algorithm in (7.35) is only approximate, so the above quantity becomes the Cramer-Rao lower
bound on the true covariance. Thus, the minimum and maximum eigenvalues of the Hessian
A can now be interpreted as the (scaled) inverse variances in the least-certain and most-certain
directions of motion. ((Steele and Jaynes 2005))


??? Image warping






